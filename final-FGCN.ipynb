{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9585,
     "status": "ok",
     "timestamp": 1647859922383,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "xAZKprjDXvrY",
    "outputId": "3702f2d7-3c34-434b-fc85-8310f6e5510f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91874b305a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1647859922385,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "fzvOqz1aX8Od"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/eksthesis-final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19166,
     "status": "ok",
     "timestamp": 1647859941533,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "eXE_qYfqc0yT",
    "outputId": "6e55c606-469e-4811-cc8e-5c11579c8123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stellargraph in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.2.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.4.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.6.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.3.5)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->stellargraph) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.24.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.0.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.5.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (13.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.17.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.13.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.44.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (57.4.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->stellargraph) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->stellargraph) (3.2.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libmetis-dev is already the newest version (5.1.0.dfsg-5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
      "Requirement already satisfied: metis in /usr/local/lib/python3.7/dist-packages (0.2a5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stellargraph\n",
    "!sudo apt-get install libmetis-dev\n",
    "!pip install metis\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1647859942255,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "G8T9AoSAwAER"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import ClusterNodeGenerator,FullBatchLinkGenerator, GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GCN,LinkEmbedding,GraphSAGE,link_classification\n",
    "from stellargraph import globalvar\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from keras.layers import Dense\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import metis\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1647859942257,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "epHgc2OqxiK6"
   },
   "outputs": [],
   "source": [
    "randomSeed = 234\n",
    "tf.random.set_seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "random.seed(randomSeed)\n",
    "\n",
    "number_of_clusters = 12  # the number of clusters/subgraphs\n",
    "clusters_per_batch = 2  # combine two cluster per batch\n",
    "random_clusters = True  # Set to False if you want to use METIS for clustering\n",
    "\n",
    "log_callback = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "log_dir = \"logs/history/\"\n",
    "tensorboard_callback=TensorBoard(log_dir=log_callback,\n",
    "                         histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True,\n",
    "                         update_freq='epoch',\n",
    "                         profile_batch=2,\n",
    "                         embeddings_freq=1)\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "executionInfo": {
     "elapsed": 1415,
     "status": "ok",
     "timestamp": 1647859943663,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "sXcyr5FDxu-I"
   },
   "outputs": [],
   "source": [
    "class GraphCLGCNLGenerator():\n",
    "    def __init__(self,G, dataset_name, model_name, batch_size = 20, num_samples = [20,10], clusters_no = 10, cpb = 2, lam =0.1,to_cluster=False):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.clusters_no = clusters_no\n",
    "        self.cpb = cpb\n",
    "        self.lam =lam\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.inital_metrices ={\"Name\":[],\"Metrices\":[]};\n",
    "        self.clusters = self.clusterGenerator(G,to_cluster)\n",
    "        self.graph = self.linkDatasetGenerator(G)\n",
    "        self.generator = self.graphGenerator(self.graph)\n",
    "        self.createBatch()\n",
    "\n",
    "    def clusterGenerator(self, graphs,randClust = False):\n",
    "        if randClust:\n",
    "            # We don't have to specify the cluster because the CluserNodeGenerator will take\n",
    "            # care of the random clustering for us.\n",
    "            self.clusters = self.clusters_no\n",
    "        else:\n",
    "            # We are going to use the METIS clustering algorith,\n",
    "            print(\"Graph clustering using the METIS algorithm.\")\n",
    "\n",
    "            lil_adj = graphs.to_adjacency_matrix().tolil()\n",
    "            adjlist = [tuple(neighbours) for neighbours in lil_adj.rows]\n",
    "\n",
    "            edgecuts, parts = metis.part_graph(adjlist, self.clusters_no)\n",
    "            parts = np.array(parts)\n",
    "            clusters = []\n",
    "            cluster_ids = np.unique(parts)\n",
    "            for cluster_id in cluster_ids:\n",
    "                mask = np.where(parts == cluster_id)\n",
    "                clusters.append(node_ids[mask])\n",
    "        return clusters\n",
    "\n",
    "    def linkDatasetGenerator(self, graphs):\n",
    "        edge_splitter_test = EdgeSplitter(graphs)\n",
    "        self.G_test, self.edge_ids_test, self.edge_labels_test = edge_splitter_test.train_test_split(\n",
    "            p=0.1, method=\"global\", keep_connected=True\n",
    "        )\n",
    "\n",
    "        edge_splitter_val = EdgeSplitter(self.G_test)\n",
    "        self.G_val, self.edge_ids_val, self.edge_labels_val = edge_splitter_val.train_test_split(\n",
    "            p=0.05, method=\"global\", keep_connected=True, seed=10\n",
    "        )\n",
    "\n",
    "        edge_splitter_train = EdgeSplitter(self.G_val)\n",
    "        self.G_train, self.edge_ids_train, self.edge_labels_train = edge_splitter_train.train_test_split(\n",
    "            p=0.15, method=\"global\", keep_connected=True, seed=200\n",
    "        )\n",
    "        return self.G_train\n",
    "\n",
    "    def graphGenerator(self, st_graph):\n",
    "        if self.model_name in [\"CLGCN\",\"FGCN\"]:\n",
    "            return FullBatchLinkGenerator(st_graph, method=\"gcn\")\n",
    "        elif self.model_name == \"SAGEL\":\n",
    "            return GraphSAGELinkGenerator(st_graph, self.batch_size, self.num_samples)\n",
    "\n",
    "\n",
    "    def createModel(self,input_shape=[20,10], act_fun=\"relu\",drp=0.5):\n",
    "        self.dropout=drp\n",
    "        self.input_shape = str(input_shape[0])+str(input_shape[1])\n",
    "        if self.model_name in [\"FGCN\",\"CLGCN\"]:\n",
    "            graph_network = GCN(\n",
    "                layer_sizes=input_shape, activations=[act_fun, act_fun], generator=self.generator, dropout=drp\n",
    "            )\n",
    "            x_inp, x_out = graph_network.in_out_tensors()\n",
    "            prediction = LinkEmbedding(activation=\"relu\", method=\"ip\")(x_out)\n",
    "\n",
    "\n",
    "        elif self.model_name == \"SAGEL\":\n",
    "            graph_network = GraphSAGE(layer_sizes=input_shape, generator=self.generator, bias=True, dropout=drp)\n",
    "            x_inp, x_out = graph_network.in_out_tensors()\n",
    "            prediction = link_classification(\n",
    "                output_dim=1, output_act=\"relu\", edge_embedding_method=\"ip\"\n",
    "            )(x_out)\n",
    "\n",
    "        self.model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    def optimize(self,optimizer = \"ADAM\", lr=0.001, loss = \"keras.losses.binary_crossentropy\", metrics = [\"acc\"]):\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        if optimizer == \"ADAM\":\n",
    "            self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        self.loss = loss\n",
    "        self.metric = metrics\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss,\n",
    "            metrics=self.metric,\n",
    "        )\n",
    "    def storeMetric(self,name, stage , data_flow):\n",
    "        initial_metrics = self.model.evaluate(data_flow)\n",
    "        self.inital_metrices[\"Name\"].append(name+\" \"+stage)\n",
    "        self.inital_metrices[\"Name\"].append(initial_metrics)\n",
    "        self.inital_metrices[\"Metrices\"].append(self.model.metrics_names)\n",
    "\n",
    "        print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "        for name, val in zip(self.model.metrics_names,initial_metrics):\n",
    "            print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "\n",
    "    def createBatch(self):\n",
    "        batches = []\n",
    "        q= self.cpb\n",
    "        if q > 1:\n",
    "            # combine clusters\n",
    "            cluster_indices = list(range(len(self.clusters)))\n",
    "            random.shuffle(cluster_indices)\n",
    "\n",
    "            for i in range(0, len(cluster_indices) - 1, q):\n",
    "                cc = cluster_indices[i: i + q]\n",
    "                tmp = []\n",
    "                for l in cc:\n",
    "                    tmp.extend(list(self.clusters[l]))\n",
    "                batches.append(tmp)\n",
    "        else:\n",
    "            batches = copy.deepcopy(self.clusters)\n",
    "        self.batches = batches\n",
    "\n",
    "    def getCluster(self,i,target_edges_ids,edges_label):\n",
    "        g_node_list = list(self.batches[i])\n",
    "        target_edges_ids = target_edges_ids.tolist()\n",
    "        edges_label = edges_label.tolist()\n",
    "        flatten_node_ids = np.asarray(target_edges_ids).reshape(-1)\n",
    "        unique_nodes = np.unique(flatten_node_ids)\n",
    "        target_nodes_cluster = list(set(list(unique_nodes)).intersection(set(g_node_list)))\n",
    "        # find edges_ids and edges label in batch\n",
    "        cluster_edges_ids = []\n",
    "        cluster_edges_label = []\n",
    "        for index, edge in enumerate(target_edges_ids):\n",
    "            if set(edge).issubset(set(target_nodes_cluster)):\n",
    "                cluster_edges_ids.append(edge)\n",
    "                cluster_edges_label.append(edges_label[index])\n",
    "        cluster_edges_ids = np.asarray(cluster_edges_ids)\n",
    "        cluster_edges_label = np.asarray(cluster_edges_label)\n",
    "\n",
    "        return cluster_edges_ids, cluster_edges_label\n",
    "\n",
    "    def storeInstance(self,hstry,time_data):\n",
    "        self.folder_name = log_dir+self.model_name+\"/\"+self.dataset_name.upper()+\"/\"\n",
    "        if not os.path.exists(self.folder_name):\n",
    "            os.makedirs(self.folder_name)\n",
    "        # try:\n",
    "        #     # Create target Directory\n",
    "        #     os.makedirs(folder_name)\n",
    "        #     print(\"Directory \", folder_name, \" Created \")\n",
    "        # except FileExistsError:\n",
    "        #     print(\"Directory \",folder_name, \" already exists\")\n",
    "        filenames=self.folder_name+\"history-lr=\"+str(self.learning_rate)+\"-drop=\"+str(self.dropout)+\"-input=\"+str(self.input_shape)+\".csv\"\n",
    "        datas =pd.DataFrame(hstry.history)\n",
    "        datas[\"time\"]=time_data\n",
    "        datas.to_csv(filenames)\n",
    "\n",
    "        # with open(filename, 'wb') as file_pi:\n",
    "        #     pickle.dump(hstry.history, file_pi)\n",
    "\n",
    "\n",
    "    def train(self,epochs):\n",
    "        train_gen = self.graphGenerator(self.G_train)\n",
    "        val_gen = self.graphGenerator(self.G_val)\n",
    "        timer = AverageMeter(\"Timer\")\n",
    "        if self.model_name in [\"FGCN\",\"SAGEL\"]:\n",
    "            train_flow = train_gen.flow(self.edge_ids_train, self.edge_labels_train)\n",
    "            val_flow = val_gen.flow(self.edge_ids_val, self.edge_labels_val)\n",
    "            time_callback = TimeHistory()\n",
    "            start = time.time()\n",
    "            history = self.model.fit(\n",
    "                train_flow, epochs=epochs, validation_data=val_flow, verbose=1, shuffle=False, callbacks=[tensorboard_callback,time_callback]\n",
    "            )\n",
    "            timer.update(time.time() - start)\n",
    "            times = time_callback.times\n",
    "            history.params[\"time\"]=timer.sum\n",
    "            self.storeInstance(history,times)\n",
    "            print(\"Total Time Elapsed = {0:.4f}\".format(timer.sum))\n",
    "\n",
    "        else:\n",
    "            hist = None\n",
    "            once= True\n",
    "            max_accuracy = 0\n",
    "            time_arr = []\n",
    "            for epoch in range(epochs):\n",
    "                trainLosses = AverageMeter('Loss')\n",
    "                trainAcc = AverageMeter('Acc@1')\n",
    "                valLosses = AverageMeter('Loss')\n",
    "                valAcc = AverageMeter('Acc@1')\n",
    "                local_timer = 0\n",
    "\n",
    "\n",
    "                for i in range(len(self.batches)):\n",
    "                    train_data = self.getCluster(i, self.edge_ids_train, self.edge_labels_train)\n",
    "                    val_data = self.getCluster(i, self.edge_ids_val, self.edge_labels_val)\n",
    "                    train_flow = train_gen.flow(train_data[0], train_data[1])\n",
    "                    val_flow = val_gen.flow(val_data[0], val_data[1])\n",
    "                    start = time.time()\n",
    "                    history = self.model.fit(\n",
    "                        train_flow, validation_data=val_flow, verbose=0, shuffle=True\n",
    "                    )\n",
    "                    temp_time = time.time()-start\n",
    "                    local_timer+=temp_time\n",
    "                    timer.update(temp_time)\n",
    "                    trainAcc.update(history.history['acc'][0],len(train_data[1]))\n",
    "                    trainLosses.update(history.history['loss'][0],len(train_data[1]))\n",
    "                    valAcc.update(history.history['val_acc'][0], len(val_data[1]))\n",
    "                    valLosses.update(history.history['val_loss'][0], len(val_data[1]))\n",
    "\n",
    "\n",
    "                if once:\n",
    "                    hist= copy.deepcopy(history)\n",
    "                    hist.history['acc'][0]=trainAcc.avg\n",
    "                    hist.history['loss'][0] = trainLosses.avg\n",
    "                    hist.history['val_acc'][0] = valAcc.avg\n",
    "                    hist.history['val_loss'][0] = valLosses.avg\n",
    "                    once = False\n",
    "                else:\n",
    "                    hist.epoch.append(epoch)\n",
    "                    hist.history['acc'].append(trainAcc.avg)\n",
    "                    hist.history['loss'].append(trainLosses.avg)\n",
    "                    hist.history['val_acc'].append(valAcc.avg)\n",
    "                    hist.history['val_loss'].append(valLosses.avg)\n",
    "\n",
    "                if trainAcc.avg>max_accuracy:\n",
    "                    max_accuracy = trainAcc.avg\n",
    "                    if not os.path.exists(log_callback):\n",
    "                        os.mkdir(log_callback)\n",
    "\n",
    "                    #self.model.save(log_callback)\n",
    "\n",
    "\n",
    "                print(\"Epoch \"+str(epoch+1)+\"/\"+str(epochs))\n",
    "                time_arr.append(local_timer)\n",
    "                print(\"1/1 [=============================] - {0:2.0f}s {0:0.4f}ms/step - loss: {1:f} - acc: {2:f} - val_loss: {3:f} - val_acc: {4:f}\".format(local_timer,trainLosses.avg,trainAcc.avg,valLosses.avg,valAcc.avg))\n",
    "            hist.params[\"Time\"]=timer.sum\n",
    "            hist.params[\"epochs\"] = epochs\n",
    "            self.storeInstance(hist,time_arr)\n",
    "            print(hist.params)\n",
    "            print(\"Total Time Elapsed = {0:.4f}\".format(timer.sum))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, G_graph,edges,labels):\n",
    "        test_gen = self.graphGenerator(G_graph)\n",
    "        self.test_flow = test_gen.flow(edges, labels)\n",
    "        self.model.evaluate(self.test_flow)\n",
    "\n",
    "    def get_f1_score(self,y_true, y_pred):\n",
    "\n",
    "        y_true = np.array(y_true, dtype=np.float64)\n",
    "        y_pred = np.array(y_pred, dtype=np.float64)\n",
    "\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "        return f1_val\n",
    "\n",
    "    def metrices(self, ytrue, ypred):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        score = self.get_f1_score(ytrue, ypred)\n",
    "        nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(ytrue, ypred)\n",
    "        auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
    "\n",
    "        filename = self.folder_name+\"f1_roc-lr=\"+str(self.learning_rate)+\"-drop=\"+str(self.dropout)+\"-input=\"+str(self.input_shape)+\".csv\"\n",
    "        datas_auc = pd.DataFrame(columns=['F1','AUC'])\n",
    "        datas_auc.loc[len(datas_auc)]=[float(score),float(auc_keras)]\n",
    "        datas_auc.to_csv(filename)\n",
    "        print('F1 Score for {}-{} : {:.4f}'.format(self.model_name,self.dataset_name,score))\n",
    "        print('Area under ROC for {}-{} : {:.4f}'.format(self.model_name,self.dataset_name,auc_keras))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5661,
     "status": "error",
     "timestamp": 1647860122030,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "ekLGpG6gx5jj",
    "outputId": "27d97de5-83cd-47e3-854c-3f5dc337a24b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph clustering using the METIS algorithm.\n",
      "** Sampled 542 positive and 542 negative edges. **\n",
      "** Sampled 244 positive and 244 negative edges. **\n",
      "** Sampled 696 positive and 696 negative edges. **\n",
      "Using GCN (local pooling) filters...\n",
      "FGCN cora\n",
      "Using GCN (local pooling) filters...\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1936 - acc: 0.5029 - val_loss: 7.5494 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 7.4990 - acc: 0.5014 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 7.6246 - acc: 0.5000 - val_loss: 7.6246 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.6246 - acc: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-140-316e7ea9b141>\", line 31, in <module>\n",
      "    model.train(100)\n",
      "  File \"<ipython-input-133-18ff9fc23dda>\", line 171, in train\n",
      "    train_flow, epochs=epochs, validation_data=val_flow, verbose=1, shuffle=False, callbacks=[tensorboard_callback,time_callback]\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 475, in relpath\n",
      "    start_list = [x for x in abspath(start).split(sep) if x]\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "dataset = \"cora\"  # can also select 'pubmed'\n",
    "\n",
    "if dataset == \"cora\":\n",
    "    G, labels = datasets.Cora().load()\n",
    "    display(HTML(datasets.Cora().description))\n",
    "elif dataset == \"pubmed\":\n",
    "    G, labels = datasets.PubMedDiabetes().load()\n",
    "    display(HTML(datasets.PubMedDiabetes().description))\n",
    "elif dataset == \"citeseer\":\n",
    "    data = datasets.CiteSeer()\n",
    "    G, node_subjects = data.load(largest_connected_component_only=True)\n",
    "    display(HTML(datasets.CiteSeer().description))\n",
    "G.info()\n",
    "\n",
    "if dataset == \"cora\":\n",
    "    train_size = 140\n",
    "elif dataset == \"pubmed\":\n",
    "    train_size = 60\n",
    "\n",
    "node_ids = np.array(G.nodes())\n",
    "\n",
    "\n",
    "model = GraphCLGCNLGenerator(G,model_name=\"FGCN\",dataset_name=dataset, clusters_no = 10, cpb = 2, lam =0.1)\n",
    "\n",
    "loss_fun = keras.losses.binary_crossentropy\n",
    "model.createModel(input_shape =[64,64],drp=0.3)\n",
    "\n",
    "model.optimize(\"ADAM\",1e-2, loss_fun, [\"acc\"])\n",
    "#model.model.summary()\n",
    "print(model.model_name,model.dataset_name)\n",
    "model.train(100)\n",
    "\n",
    "model.test(model.G_test,model.edge_ids_test,model.edge_labels_test)\n",
    "pred = model.model.predict(model.test_flow).ravel()\n",
    "model.metrices(model.edge_labels_test,pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4341,
     "status": "ok",
     "timestamp": 1647860085393,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "HMc1aJN_dcNn",
    "outputId": "aad343cc-f73d-4c0a-9303-f78489d9c0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history-lr=0.001-drop=0.3-input=2010.csv\n",
      "f1_roc-lr=0.001-drop=0.3-input=2010.csv\n",
      "history-lr=0.001-drop=0.4-input=2010.csv\n",
      "f1_roc-lr=0.001-drop=0.4-input=2010.csv\n",
      "history-lr=0.001-drop=0.35-input=2010.csv\n",
      "f1_roc-lr=0.001-drop=0.35-input=2010.csv\n",
      "history-lr=0.001-drop=0.45-input=2010.csv\n",
      "f1_roc-lr=0.001-drop=0.45-input=2010.csv\n",
      "history-lr=0.001-drop=0.5-input=2010.csv\n",
      "f1_roc-lr=0.001-drop=0.5-input=2010.csv\n",
      "history-lr=0.01-drop=0.3-input=2010.csv\n",
      "f1_roc-lr=0.01-drop=0.3-input=2010.csv\n",
      "history-lr=0.0001-drop=0.3-input=2010.csv\n",
      "f1_roc-lr=0.0001-drop=0.3-input=2010.csv\n",
      "history-lr=0.0001-drop=0.35-input=2010.csv\n",
      "f1_roc-lr=0.0001-drop=0.35-input=2010.csv\n",
      "history-lr=0.01-drop=0.35-input=2010.csv\n",
      "f1_roc-lr=0.01-drop=0.35-input=2010.csv\n",
      "history-lr=0.01-drop=0.4-input=2010.csv\n",
      "f1_roc-lr=0.01-drop=0.4-input=2010.csv\n",
      "history-lr=0.0001-drop=0.4-input=2010.csv\n",
      "f1_roc-lr=0.0001-drop=0.4-input=2010.csv\n",
      "history-lr=0.0001-drop=0.45-input=2010.csv\n",
      "f1_roc-lr=0.0001-drop=0.45-input=2010.csv\n",
      "history-lr=0.01-drop=0.45-input=2010.csv\n",
      "f1_roc-lr=0.01-drop=0.45-input=2010.csv\n",
      "history-lr=0.01-drop=0.5-input=2010.csv\n",
      "f1_roc-lr=0.01-drop=0.5-input=2010.csv\n",
      "history-lr=0.0001-drop=0.5-input=2010.csv\n",
      "f1_roc-lr=0.0001-drop=0.5-input=2010.csv\n",
      "history-lr=0.001-drop=0.3-input=1616.csv\n",
      "f1_roc-lr=0.001-drop=0.3-input=1616.csv\n",
      "history-lr=0.001-drop=0.4-input=1616.csv\n",
      "f1_roc-lr=0.001-drop=0.4-input=1616.csv\n",
      "history-lr=0.01-drop=0.45-input=1616.csv\n",
      "f1_roc-lr=0.01-drop=0.45-input=1616.csv\n",
      "history-lr=0.01-drop=0.45-input=3232.csv\n",
      "f1_roc-lr=0.01-drop=0.45-input=3232.csv\n",
      "history-lr=0.01-drop=0.5-input=3232.csv\n",
      "f1_roc-lr=0.01-drop=0.5-input=3232.csv\n",
      "history-lr=0.01-drop=0.5-input=6464.csv\n",
      "f1_roc-lr=0.01-drop=0.5-input=6464.csv\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/content/drive/My Drive/eksthesis-final/logs/history/FGCN\")\n",
    "!python plot.py\n",
    "os.chdir(\"/content/drive/My Drive/eksthesis-final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1647860088779,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "wYNNrDhjyCp3"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1647860101015,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "XwzPyjqe1nBh",
    "outputId": "37cf1274-6a98-4da0-be18-9dbffed6880f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final-CLGCN.ipynb  final-SAGEL.ipynb  main.py\t   requirements.txt\n",
      "final-FGCN.ipynb   history.zip\t      plot.py\t   requirement.txt\n",
      "final.ipynb\t   logs\t\t      __pycache__  source.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1647859947006,
     "user": {
      "displayName": "Ekesh khadka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAdX7hJBkl7X_FQjMx6w1gOmBAOkTpqyjOeYKsHQ=s64",
      "userId": "13216269466848101467"
     },
     "user_tz": -345
    },
    "id": "4qjv7iWEhmWt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final-FGCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
